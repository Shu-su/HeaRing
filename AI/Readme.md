# 위험 상황 판단 AI 개발

AI 개발은 다음 단계로 진행되었습니다. 잘 정리된 PPT 이미지를 함께 첨부합니다.<br>
- 데이터 수집
- 데이터 전처리
- 데이터 수작업 정제
- 데이터 증강
- KoBert 모델을 이용한 훈련 진행
- 완성된 모델 Hugging Face Hub에 탑재
- AWS 딥러닝 EC2 생성 & 서버에 모델 배포
<br>

---

## 위험 상황 판단 위해 NLP 분류 'Bert' 모델 선택
<img width='80%' src='https://github.com/user-attachments/assets/32ee022c-433f-417a-8b0b-8ba38f36d1ee'>
<br><br>

---

## 발화 데이터셋 구축
<img width='80%' src='https://github.com/user-attachments/assets/149247f3-8345-4a90-859b-dfbb7254fa5b'><br><br>

### 1️⃣ 데이터 수집
> 데이터 수집은 AI Hub에서 제공하는 데이터로 정상 발화 데이터셋을 구축하였습니다. <br>
> 위험 발화의 경우, **치매 환자의 발화를 수집**하는 것이 어려웠기에 논문 및 유튜브를 통해 수집하며 직접 구축하였습니다.
<br>

### 2️⃣ 데이터 전처리 & 수작업 정제
> 첨부된 코드 **'1_텍스트 데이터 전처리.ipynb'를 통해 1차 전처리**를 먼저 진행하였습니다. <br>
> ⇒ 사용한 라이브러리 : 중복제거(drop_duplicates), 띄어쓰기 처리(PyKoSpacing), 맞춤법 검사(py-hanspell), 불용어 제거(정규표현식) <br>
> 전처리된 결과물을 **직접 검토하며 적절한 데이터를 선별**하고, 전처리되지 않은 문장들을 전처리해주었습니다.
<br>

### 3️⃣ 위험 발화 데이터 증강
> 수집된 위험 발화 데이터 개수가 '1,340개'로 매우 적었기에 증강 작업을 거쳤습니다. <br>
> 데이터의 길이가 짧은 것도 많았기에 바로 라이브러리를 이용한 증강을 진행하지 않고, **ChatGPT를 이용한 수작업 증강**을 진행하였습니다. <br>
> 이후 **KorEDA**를 이용하여 2차 증강을 진행해주었습니다.
> 그 결과, **4.5배**로 데이터의 개수를 늘릴 수 있었습니다.
<br>

---
## 모델 훈련
<img width='80%' src='https://github.com/user-attachments/assets/fdb09a9f-a6d5-49d3-b67e-0462c6cf60a0'>
<br><br>

> 구축된 데이터를 KoBert 모델에 넣고, 파인튜닝하여 위험상황 판단 AI 모델을 만들었습니다. <br>
> 직접 문장을 넣어 테스트해볼 때도 잘 작동하는 걸 볼 수 있었습니다. <br>
> 훈련 중 과적합이 발생하였기에, **드롭아웃 비율을 0.7로 높이고 초기 학습률을 낮추어 해결**하였습니다.
<br>

---

## 모델 성능
<img width='80%' src='https://github.com/user-attachments/assets/beba0029-9b57-4cd2-9c81-2bda0d018473'>
<br><br>

> 테스트를 하다 보니 위험발화 데이터에 과적합된 양상을 보이기에 **정상발화 데이터를 추가**하여 해결하였습니다.
<br>

---

# 서버 탑재
<img width='' src='https://github.com/user-attachments/assets/3ea10949-0e9f-4657-9ccf-e74bb547408f'><br>




# 한계
1. 위험 발화 데이터 수집의 한계

2. 실제 하드웨어와 함께 연동했을 때의 
